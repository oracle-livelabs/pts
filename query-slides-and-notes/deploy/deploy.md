# Deploy file processing components

## Introduction

Oracle cloud resources like object storage, compute nodes and database instances can be used to build complex workflows. With minimal knowledge and out-of-the-box tools, you can combine them into complex configurations. Everything can be automated using custom code in scripts, stored procedures and data structures.

This lab explains how to connect cloud resources to build a simple solution for your problem.

Estimated Time: 40 minutes

### Objectives

In this lab you will:
* Create credentials and access object storage assets
* Create tables and views to track file processing
* Create procedures for data processing inside Oracle database
* Write Bash (Bourne Again Shell) scripts for file type conversions
* Create external tables based on flat files

### Prerequisites

This lab assumes you have:
* Provisioned Oracle Cloud Resources completing the first lab
* Basic knowledge of Linux Bash programming
* Intermediate Oracle SQL and PL/SQL experience

## Task 1: Create OCI credentials to access files in bucket

1. On the previous tab where ADMIN user is connected to Database Actions, click main menu â‰¡ or Database Actions in the lower-left corner, and **SQL**.

2. Grant privileges on `DBMS_CLOUD` package to **PPTXJSON** user.

    ````sql
    <copy>
    grant execute on DBMS_CLOUD to PPTXJSON;
    </copy>
    ````

3. On the cloud console, click on user menu ðŸ‘¤ in the upper-right corner, then click your **oci-username** under Profile.

4. In the lower-left Resources menu, click **Auth Tokens**.

5. Click **Generate Token**.

    - Description: PPTX to JSON

6. Click **Generate Token**. Copy this password for your records. It will not be shown again.

7. Click **Copy** and paste it in your notes. This is your Token value. Now close the dialog.

8. Switch to the browser tab where PPTXJSON user is connected to Database Actions. It should be on the SQL Worksheet.

9. Run the following code to create cloud credentials into your autonomous database using **Run Script** button or F5.

    ````sql
    <copy>
    SET DEFINE OFF;
    BEGIN
        DBMS_CLOUD.create_credential(
        credential_name => 'CLOUD_CREDENTIAL',
        username => '<oci-username>',
        password => '<token>');
    END;
    /
    </copy>
    ````

10. Test access to your PPTX bucket using the cloud credentials. This query must return the PPTX file you uploaded into LLXXX-PPTX bucket.

    ````sql
    <copy>
    SELECT * FROM table(dbms_cloud.list_objects(
            credential_name => 'CLOUD_CREDENTIAL',
            location_uri => 'https://swiftobjectstorage.<region>.oraclecloud.com/v1/<tenancy>/LLXXX-PPTX/'));
    </copy>
    ````


## Task 2: Create processing status table and view

1. Create a new table to store processed presentations.

    ````sql
    <copy>
    create table PPTX_DONE (
      PPTX_ID   NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY (CACHE 5) primary key,
      PPTX_NAME VARCHAR2(256) not null,
      PROCESSED	DATE
     );
     </copy>
     ````

2. Create a view to list all PPTX files uploaded into LLXXX-PPTX bucket, and whether or not have been processed.

    ````sql
    <copy>
    create or replace editionable view V_PPTX_DONE as
    SELECT substr(obst.object_name,1,length(obst.object_name)-5) pptx_name,
           obst.object_name,
           obst.bytes/1024/1024 size_mb,
           to_date(substr(obst.last_modified,1,18), 'DD-MON-RR HH24.MI.SS') modified,
           ppd.pptx_name pptx_done,
           ppd.processed,
           jd.json_done
      FROM table(dbms_cloud.list_objects(credential_name => 'CLOUD_CREDENTIAL',
              location_uri => 'https://swiftobjectstorage.<region>.oraclecloud.com/v1/<tenancy>/LLXXX-PPTX/')) obst
      left join PPTX_DONE ppd on substr(obst.object_name,1,length(obst.object_name)-5) = ppd.pptx_name
      left join (select unique NVL(SUBSTR(FILE_NAME, 0, INSTR(FILE_NAME, '/')-1), FILE_NAME) as JSON_DONE from JSON_FILES) jd
        on jd.JSON_DONE = substr(obst.object_name,1,length(obst.object_name)-5);
    </copy>
    ````

3. Select all records from `V_PPTX_DONE`, and notice your PPTX file hasn't been processed yet.

    ````sql
    <copy>
    select * from V_PPTX_DONE;
    </copy>
    ````


## Task 3: Create unprocessed presentations procedure

1. Create a procedure to list unprocessed presentations from LLXXX-PPTX bucket and write them in a flat file into LLXXX-JSON bucket, where they will be processed. Use **Run Script** button or F5 to run this code.

    ````sql
    <copy>
    create or replace PROCEDURE TO_PROCESS_CSV AUTHID CURRENT_USER IS
      my_blob_data BLOB;
    BEGIN
    select utl_raw.cast_to_raw (listagg(PPTX_NAME,'
    ')) into my_blob_data from V_PPTX_DONE where PPTX_DONE is NULL;
    DBMS_CLOUD.PUT_OBJECT(
         credential_name => 'CLOUD_CREDENTIAL',
         object_uri => 'https://objectstorage.<region>.oraclecloud.com/n/<tenancy>/b/LLXXX-JSON/o/to_process.csv',
         contents => my_blob_data);
    END to_process_csv;
    /
    </copy>
    ````

2. Run the `TO_PROCESS_CSV` procedure.

    ````sql
    <copy>
    exec to_process_csv;
    </copy>
    ````


## Task 4: Write and execute PPTX to JSON conversion script

1. Connect to the LLXXX-VM compute instance using SSH.

2. Go to LLPPTX-JSON folder.

    ````bash
    <copy>
    cd LLPPTX-JSON
    </copy>
    ````

3. Use a text editor on your computer (Notepad) to create a new text file with the following contents.

    ````bash
    <copy>
    #!/bin/bash

    # save current Internal Field Separator (IFS)
    SAVEIFS=IFS
    # set end of line as separator when reading files
    IFS=$(echo -en "\n\b")

    # add end of file to to_process.csv
    echo "" >> to_process.csv

    # read every entry in to_process.csv file
    while read -r p; do
    # display this entry value
      echo ${p}
    # list PPTX file corresponding to this entry
      ls ../LLPPTX-PPTX/${p}*
    # copy new PPTX to processing bucket as ZIP file
      cp ../LLPPTX-PPTX/${p}* ./${p}.zip
    # extract files
      unzip ${p}.zip -d ${p}
    # add this entry's XML file names to CSV file
      find ./${p}/ -iname '*.xml' | sed 's/^.\///g' > new_xml_files.csv
    done <to_process.csv

    # read every XML file in new_xml_files.csv file
    while read p; do
    # check if XML was already converted to JSON
      if [ ! -f ${p::-3}json ]
      then
    # if JSON file doesn't exist
        echo "No JSON file duplicate - OK."
    # display the name of the new JSON file
        echo "Converting ${p::-3}json"
    # add XML file name to CSV for external table
        echo "${p}" >> xml_files.csv
    # perform XML to JSON conversion with XQ
        cat $p | xq . > ${p::-3}json
    # add JSON file name to CSV for external table
        echo "${p::-3}json" >> json_files.csv
      else
    # if JSON file exists, do nothing
        echo "File found ${p::-3}json. Skip."
      fi
    done <new_xml_files.csv

    # revert to original IFS
    IFS=$SAVEIFS
    </copy>
    ````

4. This is the conversion script. Take some time to read the comments in order to understand the process.

5. Save this file as `convert2json.sh` on your computer, and upload it to LLXXX-JSON bucket.

6. Use the SSH connection to the LLXXX-VM compute instance to make the conversion script executable.

    ````bash
    <copy>
    chmod u+x convert2json.sh
    </copy>
    ````

7. Convert the conversion script from Windows to Linux text format.

    ````bash
    <copy>
    dos2unix convert2json.sh
    </copy>
    ````

8. Execute the conversion script.

    ````bash
    <copy>
    ./convert2json.sh
    </copy>
    ````

9. The conversion script unpacked the ZIP archive into a folder with the same name as your PPTX presentation. It converted the XML files to JSON. It also wrote all XML files as a list into the `xml_files.csv` flat file, and all JSON files into `json_files.csv` flat file.

    ````bash
    <copy>
    ls
    </copy>
    ````


## Task 5: Create the external table for JSON files

1. Switch to the browser tab where PPTXJSON user is connected to Database Actions. It should be on the SQL Worksheet.

2. Run the following code to create an external table with all JSON files in the `json_files.csv` flat file. Click **Run Script** button or F5.

    ````sql
    <copy>
    declare
     oci_bucket VARCHAR2(128) default 'https://swiftobjectstorage.<region>.oraclecloud.com/v1/<tenancy>/LLXXX-JSON/';
    begin
     dbms_cloud.create_external_table(
        table_name =>'JSON_FILES',
        credential_name =>'CLOUD_CREDENTIAL',
        file_uri_list => oci_bucket || 'json_files.csv',
        format => json_object('type' VALUE 'CSV'),
        column_list => 'FILE_NAME  VARCHAR2(256)'
    ); end;
    /
    </copy>
    ````

3. Query the records of the `JSON_FILES` external table.

    ````sql
    <copy>
    select * from JSON_FILES;
    </copy>
    ````

    You may now **proceed to the next lab**.

## Acknowledgements

- **Author** - Valentin Leonard Tabacaru
- **Last Updated By/Date** - Valentin Leonard Tabacaru, DB Product Management, January 2023

## Need help?

Please submit feedback or ask for help using our [LiveLabs Support Forum](https://community.oracle.com/tech/developers/categories/livelabsdiscussions). Please click the **Log In** button and login using your Oracle Account. Click the **Ask A Question** button to the left to start a *New Discussion* or *Ask a Question*.  Please include your workshop name and lab name.  You can also include screenshots and attach files.  Engage directly with the author of the workshop.

If you do not have an Oracle Account, click [here](https://profile.oracle.com/myprofile/account/create-account.jspx) to create one.
