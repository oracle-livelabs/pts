# Lab 3: Create the database vector tables

## Introduction

A typical RAG application requires a vector store.  Oracle Autonomous Database 23ai will be used as the vector store. In this lab, we will use documents as the source data, but you can apply these steps to other data types including audio and video.  To get a better understanding of the RAG steps and the use of Oracle AI Vector Search `DBMS_VECTOR_CHAIN` package, see the PLSQL RAG LiveLabs.

Estimated Time: 20 min

### Objectives

In this lab, you will:
* Create the vector table in Oracle Autonomous Database 23ai
* Create a procedure to store the document chunks
* Create a trigger to embed the vectors

### Prerequisites

* Environment with Oracle Autonomous Database 23ai

## Task 1: Create tables to store the document, chunks, and vectors

1. From your Autonomous Database console select Database Actions | SQL
2. Select the schema user vector
3. Create a table named *MY\_BOOKS* in the VECTOR schema. We will use this table to load the original PDF file as a BLOB. Copy the code snippet to the SQL worksheet and click **Run**.

    ``` 

    CREATE TABLE IF NOT EXISTS "VECTOR"."MY_BOOKS"
        ( 
        ID              INTEGER GENERATED BY DEFAULT ON NULL AS IDENTITY 
            ( START WITH 1 CACHE 20 ) PRIMARY KEY, 
        file_name      VARCHAR2 (900) , 
        file_size      INTEGER , 
        file_type       VARCHAR2 (100) , 
        file_content    BLOB
        ) 
        LOGGING 
    ```

4. Create a table named *VECTOR\_STORE* in the VECTOR schema. This is used to store the corresponding text chunks and embeddings in a column of vector datatype. Copy the code snippet and click **Run**.

    ``` 

    CREATE TABLE IF NOT EXISTS VECTOR_STORE
      (	"DOC_ID" NUMBER(*,0) NOT NULL ENABLE, 
      "EMBED_ID" NUMBER, 
      "EMBED_DATA" VARCHAR2(4000 BYTE), 
      "EMBED_VECTOR" VECTOR,
        FOREIGN KEY (DOC_ID) REFERENCES MY_BOOKS(ID)
      )

    ```

## Task 2: Create database procedure and trigger 

In this task you will:
a. Create a procedure `insert_my_table_row` to insert the PDF, Word, or TXT file into the MY\_BOOKS table and return the doc\_id

b. Create a trigger `trg_mybooks_vector_store_compound` to create embedding for the PDF and store it in the VECTOR\_STORE table.

1. From the Database Actions SQL Worksheet create and run the procedure `insert_my_table_row` 

    ```

    create or replace PROCEDURE insert_my_table_row(
        p_file_name IN my_books.file_name%TYPE,
        p_file_size IN my_books.file_size%TYPE,
        p_file_type IN my_books.file_type%TYPE,
        p_file_content IN my_books.file_content%TYPE,
        p_new_id OUT number
    )
    IS
        v_count NUMBER;
        v_id    number;
        new_id  number ;
    BEGIN
        -- Check if the combination of a and b already exists
        BEGIN
        SELECT id INTO new_id FROM MY_BOOKS WHERE file_name = p_file_name AND file_size = p_file_size;
        EXCEPTION WHEN NO_DATA_FOUND THEN
        INSERT INTO MY_BOOKS (file_name, file_size, file_type, file_content)
            VALUES (p_file_name, p_file_size, p_file_type, p_file_content)
            RETURNING id into new_id;
        END;
        p_new_id := new_id;
        dbms_output.put_line(new_id);
        COMMIT;
    EXCEPTION
        WHEN OTHERS THEN
            -- Exception handling here, for example, a rollback or a custom error message
                DBMS_OUTPUT.PUT_LINE('Error: ' || SQLERRM);
            --RAISE;
    END insert_my_table_row;


    ```

2.  Create the trigger `trg_mybooks_vector_store_compound`

    ```

    CREATE OR REPLACE TRIGGER trg_mybooks_vector_store_compound
    FOR INSERT ON my_books
    COMPOUND TRIGGER

        TYPE t_id_tab IS TABLE OF my_books.id%TYPE INDEX BY PLS_INTEGER;
        v_ids t_id_tab;

        AFTER EACH ROW IS
        BEGIN
            v_ids(v_ids.COUNT + 1) := :NEW.id;
        END AFTER EACH ROW;

        AFTER STATEMENT IS
        BEGIN
            FOR i IN 1 .. v_ids.COUNT LOOP
                INSERT INTO vector_store (doc_id, embed_id, embed_data, embed_vector)
                SELECT dt.id AS doc_id, 
                      et.embed_id, 
                      et.embed_data, 
                      to_vector(et.embed_vector) AS embed_vector
                FROM my_books dt
                CROSS JOIN TABLE(
                    dbms_vector_chain.utl_to_embeddings(
                        dbms_vector_chain.utl_to_chunks(
                            dbms_vector_chain.utl_to_text(dt.file_content), 
                            json('{"by":"words","max":"300","split":"sentence","normalize":"all"}')
                        ),
                        json('{"provider":"database", "model":"tinybert_model"}')
                    )
                )  t
                CROSS JOIN JSON_TABLE(
                    t.column_value, 
                    '$[*]' COLUMNS (
                        embed_id NUMBER PATH '$.embed_id',
                        embed_data VARCHAR2(4000) PATH '$.embed_data',
                        embed_vector CLOB PATH '$.embed_vector'
                    )
                ) AS et
                WHERE dt.id = v_ids(i);
            END LOOP;
        END AFTER STATEMENT;

    END trg_mybooks_vector_store_compound;


    ```

## Task 6: Generating output using LLM.

### **Send the text chunks from the vector search with the user question to the LLM and generate the response.**

In this lab we are using the Oracle Gen AI LLM service.   The LLM involves processing both the user question and relevant text excerpts to generate responses tailored specifically to the provided context. It's essential to note that the nature of the response is contingent upon the question and the LLM utilized, with a multitude of parameters available for fine-tuning to optimize response quality.

LLM prompt engineering enables you to craft input queries or instructions to create more accurate and desirable outputs.  The following PLSQL uses a SQL CURSOR and CLOBs to generate the LLM prompt based on facts from the similarity search from Oracle Database 23ai. 

In the code below we are embedding the user question, performing a vector search in the database for the relevant text chunks using a vector distance function.  We then send the text chunks to OCI GenAI to provide the response. 

For connecting to OCI GenAI we have pre-created login credentials using DBMS\_VECTOR.CREATE\_CREDENTIAL.  The syntax of establishing the connection with OCI GenAI can be found in the documentation. 

1. Select the cell and click **Run**.

    ```
    # Create a cursor
    cursor = conn23ai.cursor()
    procedure_query = """
    create or replace FUNCTION generate_text_response_gen(user_question VARCHAR2, doc_id number) RETURN CLOB IS
      messages CLOB;
      params_genai CLOB;
      output CLOB;
      message_line VARCHAR2(4000);
      message_cursor SYS_REFCURSOR;
      user_question_vec vector;
        search_query varchar2(4000);
    BEGIN

      --vectorize the user_question
      select to_vector(vector_embedding(tinybert_model USING user_question as data)) as embedding  into  user_question_vec ;
      -- Open the cursor using the provided query string
      search_query := 'SELECT EMBED_DATA from VECTOR_STORE where doc_id ='||doc_id||' ORDER BY vector_distance(EMBED_VECTOR, :user_question_vec, COSINE)'||
      ' FETCH   FIRST 10 ROWS ONLY ' ;
      OPEN message_cursor FOR search_query USING user_question_vec;

      -- Initialize messages CLOB
      messages := '';

      -- Loop through cursor results and construct messages
      LOOP
        FETCH message_cursor INTO message_line;
        EXIT WHEN message_cursor%NOTFOUND;

        -- Append message line to messages CLOB
        messages := messages || '{"message": "' || message_line || '"},' || CHR(10);
      END LOOP;
        -- Finally pass the user question
        messages := messages || '{"Qusetion": "' || user_question  || '"},' || CHR(10);
      -- Close the cursor
      CLOSE message_cursor;

      -- Remove the trailing comma and newline character
      messages := RTRIM(messages, ',' || CHR(10));

      -- Construct params JSON
      -- In order to use the ocigenai provider below we have precreated the credentials
      params_genai := '
    {
      "provider":"ocigenai",
      "credential_name": "GENAI_CRED",
      "url": "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com/20231130/actions/generateText",
      "model": "cohere.command",
      "inferenceRequest": {
        "maxTokens": 2000,
        "temperature": 1
      }

    }
    ';
    -- dbms_output.put_line(messages);
      dbms_output.put_line(to_char(user_question_vec));

      -- Call UTL function to generate text
      output := dbms_vector_chain.utl_to_generate_text(messages, json(params_genai));
      dbms_output.put_line(output);
      -- Return the generated text
      RETURN output;

    EXCEPTION
      WHEN OTHERS THEN
        RETURN SQLERRM || SQLCODE;
    END;
    """

    # Execute the procedure creation query
    cursor.execute(procedure_query)
    ```

2.  Select the doc id on which to query


   In the function  `generate_text_response_gen`,  we pass the doc\_id to select the chucks related to a PDF doucment we loaded.  This improves the acquracy of the LLM response for the question by restricting the result within the content of PDF.


    ```
    %%sql select   a.file_name, a.file_size , b.doc_id,b.count,b.avg_text_chunk_size, b.max_chunk_size  from
    my_books a, (select doc_id ,count(*)  count, round(avg(length(embed_data))) avg_text_chunk_size, max(length(embed_data)) max_chunk_size from vector_store group by  doc_id) b 
    where a.id = b.doc_id
    ```
3. Generate LLM response

    Generate the response using the query below. This calls function `generate_text_response_gen` and passes the `doc_id` from previous step

    ```
    %%sql
    select generate_text_response_gen('List specific points in the Oracle Database 23c release note', 1)  from dual
    ```

    Observe the output.  Retry by replacing the question and see how the output changes.

    We successfully performed the vector search, sent the relevant chunks to the LLM along with the question and got the answer. 

    In this lab we learned how to create a complete RAG solution using PLSQL.  The table below lists the vector functions we used.

    <table>
      <thead>
        <tr>
          <th style={{ width: "200px" }}>
            Description
          </th>
          <th>
            Function
          </th>
        </tr>
      </thead>
        <tbody>
        <tr>
          <td>
            Embedding the user question  
          </td>
          <td>
            VECTOR_EMBEDDING(tinybert_model USING user_question as data)   
          </td>
        </tr>
        <tr>
          <td>
            Do vector search on question against text chunk embeddings
          </td>
          <td>
            VECTOR_DISTANCE(EMBED_VECTOR, :user_question_vec, COSINE)
          </td>
        </tr>
        <tr>
          <td>
            Pass the result chunks and the user question to the LLM 
          </td>
          <td>
            DBMS_VECTOR_CHAIN.UTL_TO_GENERATE_TEXT(messages, json(params_genai))
          </td>
        </tr>          
        </tbody>
    </table>

    **HURRAY!!!  We just completed LAB 1, a complete implementation of RAG using PLSQL**

• In the next LAB, the APEX code is only focused on calling the package routines in the 23ai database.

• This means that these packages could then easily be called from any programming language, eg.,

    * JDBC
    * C# with OPD.NET
    * python-oracledb
    * node-oracledb
    * OCI, OCCI, ODBC, Pro*C or Pro*COBOL
    * Go, Rust, PHP, Ruby etc


You may now [proceed to the next lab](#next).


## Acknowledgements
* **Authors** - Vijay Balebail, Milton Wan, Blake Hendricks
* **Last Updated By/Date** - Milton Wan, July 2024
